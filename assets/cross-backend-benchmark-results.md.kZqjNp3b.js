import{_ as e,c as r,o as a,aj as i}from"./chunks/framework.Bz2R-749.js";const u=JSON.parse('{"title":"Cross-Backend Benchmark Results","description":"","frontmatter":{},"headers":[],"relativePath":"cross-backend-benchmark-results.md","filePath":"cross-backend-benchmark-results.md"}'),n={name:"cross-backend-benchmark-results.md"};function s(o,t,l,d,c,h){return a(),r("div",null,[...t[0]||(t[0]=[i('<h1 id="cross-backend-benchmark-results" tabindex="-1">Cross-Backend Benchmark Results <a class="header-anchor" href="#cross-backend-benchmark-results" aria-label="Permalink to â€œCross-Backend Benchmark Resultsâ€">â€‹</a></h1><p><strong>Date</strong>: 2025-11-03 14:13 (NZST) <strong>Environment</strong>: MacOS, Apple Silicon, GraalVM JDK 25 (Oracle GraalVM 25+37.1, JIT runtime enabled)</p><h2 id="summary" tabindex="-1">Summary <a class="header-anchor" href="#summary" aria-label="Permalink to â€œSummaryâ€">â€‹</a></h2><p>This document contains actual performance measurements for the Aster language execution backends.</p><h3 id="architecture-clarification" tabindex="-1">Architecture Clarification <a class="header-anchor" href="#architecture-clarification" aria-label="Permalink to â€œArchitecture Clarificationâ€">â€‹</a></h3><p><strong>IMPORTANT</strong>: The Aster language has the following architecture:</p><ul><li><strong>TypeScript Frontend</strong>: Compiles CNL source code to Core IR (JSON format)</li><li><strong>Java Backends</strong>: Execute Core IR <ol><li><strong>Truffle Backend</strong> - AST interpreter with optional GraalVM JIT compilation</li><li><strong>Pure Java Backend</strong> - Bytecode generation and JVM execution (if implemented)</li></ol></li></ul><p><strong>TypeScript is NOT an execution backend</strong> - it&#39;s purely a compiler. There is no TypeScript interpreter for Core IR.</p><p>Therefore, this benchmark compares:</p><ol><li><strong>Truffle Backend</strong> (Interpreter mode - âœ… measurements completed)</li><li><strong>Pure Java Backend</strong> (Bytecode) - âœ… measurements completed</li></ol><h2 id="benchmark-suite" tabindex="-1">Benchmark Suite <a class="header-anchor" href="#benchmark-suite" aria-label="Permalink to â€œBenchmark Suiteâ€">â€‹</a></h2><p>Four standard benchmarks running identical algorithms:</p><h3 id="_1-factorial-10-recursive-function-calls" tabindex="-1">1. Factorial(10) - Recursive Function Calls <a class="header-anchor" href="#_1-factorial-10-recursive-function-calls" aria-label="Permalink to â€œ1. Factorial(10) - Recursive Function Callsâ€">â€‹</a></h3><p><strong>Algorithm</strong>: Recursive factorial calculation <strong>Test Case</strong>: factorial(10) = 3,628,800 <strong>Iterations</strong>: 1,000 (interpreter warmup 50; JIT warmup 100 cold + 2,000 stabilization)</p><table tabindex="0"><thead><tr><th>Backend</th><th>Average Time</th><th>Relative</th><th>Status</th></tr></thead><tbody><tr><td>Truffle (Interpreter)</td><td><strong>0.018 ms</strong></td><td>Baseline</td><td>âœ… Measured</td></tr><tr><td>Pure Java (Bytecode)</td><td><strong>0.002369 ms</strong></td><td>7.6x faster</td><td>âœ… Measured</td></tr><tr><td>Truffle (GraalVM JIT)</td><td><strong>0.020 ms</strong></td><td>1.1x slower vs interpreter</td><td>âœ… Measured</td></tr></tbody></table><p><strong>Analysis</strong>: Truffle interpreter remains extremely fast for this workload (0.018ms). GraalVM JIT adds only marginal overhead (~0.020ms) because the recursion tree is shallow and already optimized in the interpreter. Pure Java bytecode still delivers the best result at 0.002369ms (~7.6x faster than interpreter), highlighting JVM bytecode efficiency for tight recursive loops.</p><hr><h3 id="_2-fibonacci-20-heavy-recursion" tabindex="-1">2. Fibonacci(20) - Heavy Recursion <a class="header-anchor" href="#_2-fibonacci-20-heavy-recursion" aria-label="Permalink to â€œ2. Fibonacci(20) - Heavy Recursionâ€">â€‹</a></h3><p><strong>Algorithm</strong>: Naive recursive Fibonacci (exponential complexity) <strong>Test Case</strong>: fib(20) = 6,765 <strong>Iterations</strong>: 100 (interpreter warmup 50; JIT warmup 100 cold + 2,000 stabilization)</p><table tabindex="0"><thead><tr><th>Backend</th><th>Average Time</th><th>Relative</th><th>Status</th></tr></thead><tbody><tr><td>Truffle (Interpreter)</td><td><strong>23.803 ms</strong></td><td>Baseline</td><td>âœ… Measured</td></tr><tr><td>Pure Java (Bytecode)</td><td><strong>0.051874 ms</strong></td><td>459x faster</td><td>âœ… Measured</td></tr><tr><td>Truffle (GraalVM JIT)</td><td><strong>27.478 ms</strong></td><td>1.2x slower vs interpreter</td><td>âœ… Measured</td></tr></tbody></table><p><strong>Analysis</strong>: Heavy recursion stresses the Truffle interpreter at 23.8ms per call, while the GraalVM JIT result (27.5ms) shows no improvement yetâ€”indicating that these kernels require additional profiling data or larger inputs before Graal can optimize them. Pure Java bytecode remains dramatically faster (0.051874ms, ~459x faster than interpreter), confirming JVM bytecodeâ€™s superiority for deep recursion.</p><hr><h3 id="_3-list-map-2-items-higher-order-functions" tabindex="-1">3. List.map(2 items) - Higher-Order Functions <a class="header-anchor" href="#_3-list-map-2-items-higher-order-functions" aria-label="Permalink to â€œ3. List.map(2 items) - Higher-Order Functionsâ€">â€‹</a></h3><p><strong>Algorithm</strong>: List.map with doubling lambda over 2-element list <strong>Test Case</strong>: [1, 2].map(x =&gt; x * 2) = [2, 4], length = 2 <strong>Iterations</strong>: 10,000 (interpreter warmup 1,000; JIT warmup 100 cold + 5,000 stabilization)</p><table tabindex="0"><thead><tr><th>Backend</th><th>Average Time</th><th>Relative</th><th>Status</th></tr></thead><tbody><tr><td>Truffle (Interpreter)</td><td><strong>0.006 ms</strong></td><td>Baseline</td><td>âœ… Measured</td></tr><tr><td>Pure Java (Bytecode)</td><td><strong>0.000550 ms</strong></td><td>10.9x faster</td><td>âœ… Measured</td></tr><tr><td>Truffle (GraalVM JIT)</td><td><strong>0.004672 ms</strong></td><td>1.3x faster vs interpreter</td><td>âœ… Measured</td></tr></tbody></table><p><strong>Analysis</strong>: Higher-order functions benefit from GraalVM JIT, dropping from 0.006ms to 0.004672ms (~1.3x faster) once the warmup completes. Pure Java bytecode continues to deliver the fastest execution (0.000550ms), but the JIT narrowing gap shows that Graal optimization helps workloads dominated by CallTarget invocation and lambda captures.</p><hr><h3 id="_4-arithmetic-simple-computation" tabindex="-1">4. Arithmetic - Simple Computation <a class="header-anchor" href="#_4-arithmetic-simple-computation" aria-label="Permalink to â€œ4. Arithmetic - Simple Computationâ€">â€‹</a></h3><p><strong>Algorithm</strong>: compute(x) = (x * 2) + (x / 3) <strong>Test Case</strong>: compute(100) = 233 <strong>Iterations</strong>: 10,000 (interpreter warmup 1,000; JIT warmup 100 cold + 5,000 stabilization)</p><table tabindex="0"><thead><tr><th>Backend</th><th>Average Time</th><th>Relative</th><th>Status</th></tr></thead><tbody><tr><td>Truffle (Interpreter)</td><td><strong>0.002 ms</strong></td><td>Baseline</td><td>âœ… Measured</td></tr><tr><td>Pure Java (Bytecode)</td><td><strong>0.000399 ms</strong></td><td>5x faster</td><td>âœ… Measured</td></tr><tr><td>Truffle (GraalVM JIT)</td><td><strong>0.001866 ms</strong></td><td>1.1x faster vs interpreter</td><td>âœ… Measured</td></tr></tbody></table><p><strong>Analysis</strong>: Arithmetic-heavy code is already efficient in the interpreter at 0.002ms. GraalVM JIT trims the average to 0.001866ms (~1.1x faster), indicating that simple numeric kernels do benefit slightly from compilation. Pure Java bytecode remains the fastest option (0.000399ms, ~5x faster than interpreter).</p><hr><h2 id="key-findings" tabindex="-1">Key Findings <a class="header-anchor" href="#key-findings" aria-label="Permalink to â€œKey Findingsâ€">â€‹</a></h2><h3 id="_1-truffle-interpreter-vs-graalvm-jit" tabindex="-1">1. Truffle Interpreter vs GraalVM JIT <a class="header-anchor" href="#_1-truffle-interpreter-vs-graalvm-jit" aria-label="Permalink to â€œ1. Truffle Interpreter vs GraalVM JITâ€">â€‹</a></h3><table tabindex="0"><thead><tr><th>Benchmark</th><th>Truffle (Interpreter)</th><th>Truffle (GraalVM JIT)</th><th>Delta vs Interpreter</th></tr></thead><tbody><tr><td>Factorial(10)</td><td>0.018 ms</td><td>0.020 ms</td><td><strong>+11% (slower)</strong></td></tr><tr><td>Fibonacci(20)</td><td>23.803 ms</td><td>27.478 ms</td><td><strong>+15% (slower)</strong></td></tr><tr><td>List.map (2 items)</td><td>0.006 ms</td><td>0.004672 ms</td><td><strong>âˆ’22% (faster)</strong></td></tr><tr><td>Arithmetic</td><td>0.002 ms</td><td>0.001866 ms</td><td><strong>âˆ’7% (faster)</strong></td></tr></tbody></table><p><strong>Insights</strong>:</p><ul><li>Interpreter mode already extracts most performance from small recursive kernels; GraalVM needs larger or longer-running workloads to justify compilation.</li><li>Higher-order and arithmetic workloads do benefit once the JIT completes a longer stabilization cycle (1.1â€“1.3x faster).</li><li>Retaining multi-phase warmups (100 cold + 2K/5K stabilization iterations) is mandatory to trigger GraalVM optimizations.</li></ul><h3 id="_2-truffle-interpreter-performance-vs-original-estimates" tabindex="-1">2. Truffle Interpreter Performance vs Original Estimates <a class="header-anchor" href="#_2-truffle-interpreter-performance-vs-original-estimates" aria-label="Permalink to â€œ2. Truffle Interpreter Performance vs Original Estimatesâ€">â€‹</a></h3><table tabindex="0"><thead><tr><th>Benchmark</th><th>Estimated</th><th>Actual</th><th>Improvement</th></tr></thead><tbody><tr><td>Factorial</td><td>15 ms</td><td>0.018 ms</td><td><strong>833x better</strong></td></tr><tr><td>Fibonacci</td><td>100 ms</td><td>23.8 ms</td><td><strong>4.2x better</strong></td></tr><tr><td>List.map</td><td>0.05 ms</td><td>0.006 ms</td><td><strong>8.3x better</strong></td></tr><tr><td>Arithmetic</td><td>0.5 ms</td><td>0.002 ms</td><td><strong>250x better</strong></td></tr></tbody></table><p>Even before JIT, the interpreter significantly outperforms early projections thanks to efficient node implementations and low dispatch overhead.</p><h3 id="_3-pure-java-bytecode-vs-truffle-backends" tabindex="-1">3. Pure Java Bytecode vs Truffle Backends <a class="header-anchor" href="#_3-pure-java-bytecode-vs-truffle-backends" aria-label="Permalink to â€œ3. Pure Java Bytecode vs Truffle Backendsâ€">â€‹</a></h3><table tabindex="0"><thead><tr><th>Benchmark</th><th>Truffle (Interpreter)</th><th>Truffle (GraalVM JIT)</th><th>Pure Java (Bytecode)</th><th>Fastest</th></tr></thead><tbody><tr><td>Factorial(10)</td><td>0.018 ms</td><td>0.020 ms</td><td><strong>0.002369 ms</strong></td><td>Pure Java</td></tr><tr><td>Fibonacci(20)</td><td>23.803 ms</td><td>27.478 ms</td><td><strong>0.051874 ms</strong></td><td>Pure Java</td></tr><tr><td>List.map (2 items)</td><td>0.006 ms</td><td>0.004672 ms</td><td><strong>0.000550 ms</strong></td><td>Pure Java</td></tr><tr><td>Arithmetic</td><td>0.002 ms</td><td>0.001866 ms</td><td><strong>0.000399 ms</strong></td><td>Pure Java</td></tr></tbody></table><p><strong>Observations</strong>:</p><ul><li>Pure Java bytecode remains the fastest backend across all workloads (5xâ€“459x faster), especially on recursion-heavy benchmarks.</li><li>GraalVM JIT narrows the gap for lambda-heavy and arithmetic workloads but still trails bytecode.</li><li>Focusing on broader stdlib coverage for the bytecode backend will unlock more realistic end-to-end comparisons.</li></ul><h3 id="_4-architecture-understanding" tabindex="-1">4. Architecture Understanding <a class="header-anchor" href="#_4-architecture-understanding" aria-label="Permalink to â€œ4. Architecture Understandingâ€">â€‹</a></h3><ul><li><strong>Truffle</strong>: Primary execution backend with interpreter + JIT modes (both measured)</li><li><strong>Pure Java Bytecode</strong>: Generates JVM classes; currently limited stdlib coverage</li><li><strong>TypeScript</strong>: Compiler frontend only; not part of runtime comparisons</li></ul><hr><h2 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to â€œNext Stepsâ€">â€‹</a></h2><h3 id="_1-broaden-pure-java-backend-coverage-ðŸ”§" tabindex="-1">1. Broaden Pure Java Backend Coverage ðŸ”§ <a class="header-anchor" href="#_1-broaden-pure-java-backend-coverage-ðŸ”§" aria-label="Permalink to â€œ1. Broaden Pure Java Backend Coverage ðŸ”§â€">â€‹</a></h3><ul><li>Implement <code>List.filter</code>, <code>List.reduce</code>, <code>Result.mapOk</code>, <code>Result.mapErr</code>, and related helpers.</li><li>Add bytecode emission + runtime tests to keep parity with interpreter semantics.</li></ul><h3 id="_2-tune-graalvm-jit-benchmarks-ðŸ§ª" tabindex="-1">2. Tune GraalVM JIT Benchmarks ðŸ§ª <a class="header-anchor" href="#_2-tune-graalvm-jit-benchmarks-ðŸ§ª" aria-label="Permalink to â€œ2. Tune GraalVM JIT Benchmarks ðŸ§ªâ€">â€‹</a></h3><ul><li>Experiment with heavier workloads (larger inputs or batched invocations) to accumulate more profiling data for GraalVM.</li><li>Capture compilation logs (e.g., via <code>--engine.TraceCompilation</code>) to verify when optimization kicks in.</li></ul><h3 id="_3-ci-integration-ðŸ“ˆ" tabindex="-1">3. CI Integration ðŸ“ˆ <a class="header-anchor" href="#_3-ci-integration-ðŸ“ˆ" aria-label="Permalink to â€œ3. CI Integration ðŸ“ˆâ€">â€‹</a></h3><ul><li>Surface interpreter, bytecode, and JIT benchmarks as informational checks (non-blocking by default).</li><li>Track deltas over time to spot performance regressions once broader coverage is available.</li></ul><hr><h2 id="test-environment-details" tabindex="-1">Test Environment Details <a class="header-anchor" href="#test-environment-details" aria-label="Permalink to â€œTest Environment Detailsâ€">â€‹</a></h2><h3 id="hardware" tabindex="-1">Hardware <a class="header-anchor" href="#hardware" aria-label="Permalink to â€œHardwareâ€">â€‹</a></h3><ul><li><strong>Platform</strong>: macOS (Darwin 25.0.0)</li><li><strong>CPU</strong>: Apple Silicon (M-series)</li><li><strong>Memory</strong>: Not measured</li></ul><h3 id="software" tabindex="-1">Software <a class="header-anchor" href="#software" aria-label="Permalink to â€œSoftwareâ€">â€‹</a></h3><ul><li><strong>JDK</strong>: Oracle GraalVM JDK 25 (25+37.1, JVMCI enabled)</li><li><strong>Truffle Modules</strong>: <code>truffle-api</code>, <code>truffle-runtime</code>, <code>truffle-compiler</code> 25.0.0</li><li><strong>Build Tool</strong>: Gradle 9.0.0</li></ul><h3 id="truffle-configuration" tabindex="-1">Truffle Configuration <a class="header-anchor" href="#truffle-configuration" aria-label="Permalink to â€œTruffle Configurationâ€">â€‹</a></h3><ul><li><strong>Context</strong>: Polyglot API with <code>allowAllAccess(true)</code></li><li><strong>Warnings</strong>: <code>engine.WarnInterpreterOnly=false</code> (suppressed after runtime enabled)</li><li><strong>Warmup Strategy</strong>: Multi-phase (cold trigger + stabilization + measurement)</li></ul><hr><h2 id="measurement-methodology" tabindex="-1">Measurement Methodology <a class="header-anchor" href="#measurement-methodology" aria-label="Permalink to â€œMeasurement Methodologyâ€">â€‹</a></h2><ul><li><strong>Timing</strong>: <code>System.nanoTime()</code>, averaged per iteration.</li><li><strong>Interpreter Baseline Warmup</strong>: 50 iterations (Factorial/Fibonacci) or 1,000 iterations (List.map/Arithmetic).</li><li><strong>JIT Warmup</strong>: <ul><li>Phase 1: 100 cold executions to trigger compilation.</li><li>Phase 2: 2,000 (Factorial/Fibonacci) or 5,000 (List.map/Arithmetic) stabilization iterations.</li><li>Phase 3: Measurement loops matching interpreter counts (1,000 / 100 / 10,000 / 10,000).</li></ul></li><li><strong>Validation</strong>: Each iteration asserts expected results; runs are isolated per benchmark.</li></ul><hr><h2 id="conclusion" tabindex="-1">Conclusion <a class="header-anchor" href="#conclusion" aria-label="Permalink to â€œConclusionâ€">â€‹</a></h2><ul><li>The Truffle interpreter continues to deliver outstanding baseline performance, far exceeding early estimates.</li><li>GraalVM JIT currently yields modest gains (up to ~22%) on small kernels; larger or longer-running workloads are needed to showcase its full potential on Aster.</li><li>Pure Java bytecode remains the throughput leader, underscoring the value of extending backend coverage and keeping both implementations in sync.</li></ul><p><strong>Recommendations</strong>:</p><ol><li>Prioritize stdlib parity for the Pure Java backend so more programs can be exercised across all three execution modes.</li><li>Iterate on JIT benchmarks with heavier workloads and tracing to confirm optimization thresholds.</li><li>Integrate the benchmark suites into CI as informational jobs to catch regressions early.</li></ol><hr><p><strong>Document Version</strong>: 1.1<br><strong>Last Updated</strong>: 2025-11-03<br><strong>Author</strong>: Codex (GraalVM benchmark run)<br><strong>Review Status</strong>: Draft (JIT measurements added; awaiting backend coverage expansion)</p>',72)])])}const g=e(n,[["render",s]]);export{u as __pageData,g as default};
